MCTS Algorithm

# (s, a) = (state, action)
Each Node in the search tree contains edges (s, a) for all legal actions a from that state s
Each edge (s,a) stores the following info: [N(s, a), W(s, a), Q(s, a), P(s, a)]
  P(s, a) - prior probability
  N(s, a) - visit count
  Q(s, a) - mean action value
  W(s, a) - total action value

Each simulation starts from he root state and iteratively selects moves that maximize the search criteria (see below), until a leaf node s' is encountered.

Search criteria:
  Q(s, a) + U(s, a), where U(s, a) ∝ P(s, a) / (1 + N(s, a))
                                   ^only proportional b/c of exploration
                                    constant
Once we hit a leaf, we expand and evaluate once by the network to generate prior probabilities and evaluation (P(s',·), V(s')) = f_theta(s'). in english, P(s') and V(s') is found by pushing the leaf's expansion through the net.
Each edge that is traversed in this simulation is updated to increment it's visit count N(s, a) and to update its action value to the mean evaluations over this simulation:
  Q(s, a) = (1/N(s, a))*(sum(all evaluations v in the path from s to s'))

Algorithm:

1. Using search criteria (pick highest Q+U), step forward
2. Once you hit a leaf, i.e. decide to explore an edge that is expandable, you expand the state that the edge points to
  A leaf is a node that is 'expandable' which means that it is nonterminal and has unexplored children.
  Expanding a node means we add one or more child nodes to the tree from this node depending on our tree policy or search criteria.

  Think of a node as a state. The node holds outgoing edges, which store a state and the action took to get to that state. A state is expanded when we enumerate all legal moves

3. Backpropagate the values back to the root, tracing the path we took.
  All edges we traversed get updated as we go back: N is incremented, Q is really average cumulative action evaluation, and W is changed because it is just cumulative action eval of everything below it.

Expand and eval:
  leaf node s_L is added to a queue for neural network evaluation.
  the net actually evaluates



Node:
  - state
  - edges: [edge]
Edge:
  - action (move that got us here) --- a
  - Node: state (that the edge goes to)  --- s
  - visit count --- N
  - probability of the action we took to get here --- P
  - mean action value (average of all state evaluations below the edge) --- Q function?
  - total action value (working sum of all state evaluations below edge) --- W
